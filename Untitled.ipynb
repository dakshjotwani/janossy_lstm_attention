{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, Dataset, BucketIterator\n",
    "from torchtext.datasets import TranslationDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train.py'''\n",
    "\n",
    "# import transformer.Constants as Constants\n",
    "# from transformer.Models import Transformer\n",
    "# from transformer.Optim import ScheduledOptim\n",
    "from Transformer import Constants, Transformer, ScheduledOptim\n",
    "\n",
    "def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word\n",
    "\n",
    "\n",
    "def cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(trg_pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def patch_src(src, pad_idx):\n",
    "    src = src.transpose(0, 1)\n",
    "    return src\n",
    "\n",
    "\n",
    "def patch_trg(trg, pad_idx):\n",
    "    trg = trg.transpose(0, 1)\n",
    "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
    "    return trg, gold\n",
    "\n",
    "\n",
    "def train_epoch(model, training_data, optimizer, opt, device, smoothing):\n",
    "    ''' Epoch operation in training phase'''\n",
    "\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "        # prepare data\n",
    "        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        # note keeping\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "def eval_epoch(model, validation_data, device, opt):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    desc = '  - (Validation) '\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "            # prepare data\n",
    "            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "            trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "            # forward\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, opt.trg_pad_idx, smoothing=False)\n",
    "\n",
    "            # note keeping\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "def train(model, training_data, validation_data, optimizer, device, opt):\n",
    "    ''' Start training '''\n",
    "\n",
    "    log_train_file, log_valid_file = None, None\n",
    "\n",
    "    if opt.log:\n",
    "        log_train_file = opt.log + '.train.log'\n",
    "        log_valid_file = opt.log + '.valid.log'\n",
    "\n",
    "        print('[Info] Training performance will be written to file: {} and {}'.format(\n",
    "            log_train_file, log_valid_file))\n",
    "\n",
    "        with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n",
    "            log_tf.write('epoch,loss,ppl,accuracy\\n')\n",
    "            log_vf.write('epoch,loss,ppl,accuracy\\n')\n",
    "\n",
    "    def print_performances(header, loss, accu, start_time):\n",
    "        print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n",
    "              'elapse: {elapse:3.3f} min'.format(\n",
    "                  header=f\"({header})\", ppl=math.exp(min(loss, 100)),\n",
    "                  accu=100*accu, elapse=(time.time()-start_time)/60))\n",
    "\n",
    "    #valid_accus = []\n",
    "    valid_losses = []\n",
    "    for epoch_i in range(opt.epoch):\n",
    "        print('[ Epoch', epoch_i, ']')\n",
    "\n",
    "        start = time.time()\n",
    "        train_loss, train_accu = train_epoch(\n",
    "            model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n",
    "        print_performances('Training', train_loss, train_accu, start)\n",
    "\n",
    "        start = time.time()\n",
    "        valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n",
    "        print_performances('Validation', valid_loss, valid_accu, start)\n",
    "\n",
    "        valid_losses += [valid_loss]\n",
    "\n",
    "        checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n",
    "\n",
    "        if opt.save_model:\n",
    "            if opt.save_mode == 'all':\n",
    "                model_name = opt.save_model + '_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n",
    "                torch.save(checkpoint, model_name)\n",
    "            elif opt.save_mode == 'best':\n",
    "                model_name = opt.save_model + '.chkpt'\n",
    "                if valid_loss <= min(valid_losses):\n",
    "                    torch.save(checkpoint, model_name)\n",
    "                    print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "        if log_train_file and log_valid_file:\n",
    "            with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n",
    "                log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                    epoch=epoch_i, loss=train_loss,\n",
    "                    ppl=math.exp(min(train_loss, 100)), accu=100*train_accu))\n",
    "                log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                    epoch=epoch_i, loss=valid_loss,\n",
    "                    ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu))\n",
    "\n",
    "def main():\n",
    "    ''' \n",
    "    Usage:\n",
    "    python train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -save_model trained -b 256 -warmup 128000\n",
    "    '''\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('-data_pkl', default=None)     # all-in-1 data pickle or bpe field\n",
    "\n",
    "    parser.add_argument('-train_path', default=None)   # bpe encoded data\n",
    "    parser.add_argument('-val_path', default=None)     # bpe encoded data\n",
    "\n",
    "    parser.add_argument('-epoch', type=int, default=10)\n",
    "    parser.add_argument('-b', '--batch_size', type=int, default=2048)\n",
    "\n",
    "    parser.add_argument('-d_model', type=int, default=512)\n",
    "    parser.add_argument('-d_inner_hid', type=int, default=2048)\n",
    "    parser.add_argument('-d_k', type=int, default=64)\n",
    "    parser.add_argument('-d_v', type=int, default=64)\n",
    "\n",
    "    parser.add_argument('-n_head', type=int, default=8)\n",
    "    parser.add_argument('-n_layers', type=int, default=6)\n",
    "    parser.add_argument('-warmup','--n_warmup_steps', type=int, default=4000)\n",
    "\n",
    "    parser.add_argument('-dropout', type=float, default=0.1)\n",
    "    parser.add_argument('-embs_share_weight', action='store_true')\n",
    "    parser.add_argument('-proj_share_weight', action='store_true')\n",
    "\n",
    "    parser.add_argument('-log', default=None)\n",
    "    parser.add_argument('-save_model', default=None)\n",
    "    parser.add_argument('-save_mode', type=str, choices=['all', 'best'], default='best')\n",
    "\n",
    "    parser.add_argument('-no_cuda', action='store_true')\n",
    "    parser.add_argument('-label_smoothing', action='store_true')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    opt.cuda = not opt.no_cuda\n",
    "    opt.d_word_vec = opt.d_model\n",
    "\n",
    "    if not opt.log and not opt.save_model:\n",
    "        print('No experiment result will be saved.')\n",
    "        raise\n",
    "\n",
    "    if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n",
    "        print('[Warning] The warmup steps may be not enough.\\n'\\\n",
    "              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n",
    "              'Using smaller batch w/o longer warmup may cause '\\\n",
    "              'the warmup stage ends with only little data trained.')\n",
    "\n",
    "    device = torch.device('cuda' if opt.cuda else 'cpu')\n",
    "\n",
    "    #========= Loading Dataset =========#\n",
    "\n",
    "    if all((opt.train_path, opt.val_path)):\n",
    "        training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\n",
    "    elif opt.data_pkl:\n",
    "        training_data, validation_data = prepare_dataloaders(opt, device)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    print(opt)\n",
    "\n",
    "    transformer = Transformer(\n",
    "        opt.src_vocab_size,\n",
    "        opt.trg_vocab_size,\n",
    "        src_pad_idx=opt.src_pad_idx,\n",
    "        trg_pad_idx=opt.trg_pad_idx,\n",
    "        trg_emb_prj_weight_sharing=opt.proj_share_weight,\n",
    "        emb_src_trg_weight_sharing=opt.embs_share_weight,\n",
    "        d_k=opt.d_k,\n",
    "        d_v=opt.d_v,\n",
    "        d_model=opt.d_model,\n",
    "        d_word_vec=opt.d_word_vec,\n",
    "        d_inner=opt.d_inner_hid,\n",
    "        n_layers=opt.n_layers,\n",
    "        n_head=opt.n_head,\n",
    "        dropout=opt.dropout).to(device)\n",
    "\n",
    "    optimizer = ScheduledOptim(\n",
    "        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        2.0, opt.d_model, opt.n_warmup_steps)\n",
    "\n",
    "    train(transformer, training_data, validation_data, optimizer, device, opt)\n",
    "\n",
    "\n",
    "def prepare_dataloaders_from_bpe_files(opt, device):\n",
    "    batch_size = opt.batch_size\n",
    "    MIN_FREQ = 2\n",
    "    if not opt.embs_share_weight:\n",
    "        raise\n",
    "\n",
    "    data = pickle.load(open(opt.data_pkl, 'rb'))\n",
    "    MAX_LEN = data['settings'].max_len\n",
    "    field = data['vocab']\n",
    "    fields = (field, field)\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    train = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=opt.train_path, \n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "    val = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=opt.val_path, \n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "\n",
    "    opt.max_token_seq_len = MAX_LEN + 2\n",
    "    opt.src_pad_idx = opt.trg_pad_idx = field.vocab.stoi[Constants.PAD_WORD]\n",
    "    opt.src_vocab_size = opt.trg_vocab_size = len(field.vocab)\n",
    "\n",
    "    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n",
    "    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n",
    "    return train_iterator, val_iterator\n",
    "\n",
    "\n",
    "def prepare_dataloaders(opt, device):\n",
    "    batch_size = opt.batch_size\n",
    "    data = pickle.load(open(opt.data_pkl, 'rb'))\n",
    "\n",
    "    opt.max_token_seq_len = data['settings'].max_len\n",
    "    opt.src_pad_idx = data['vocab']['src'].vocab.stoi[Constants.PAD_WORD]\n",
    "    opt.trg_pad_idx = data['vocab']['trg'].vocab.stoi[Constants.PAD_WORD]\n",
    "\n",
    "    opt.src_vocab_size = len(data['vocab']['src'].vocab)\n",
    "    opt.trg_vocab_size = len(data['vocab']['trg'].vocab)\n",
    "\n",
    "    #========= Preparing Model =========#\n",
    "    if opt.embs_share_weight:\n",
    "        assert data['vocab']['src'].vocab.stoi == data['vocab']['trg'].vocab.stoi, \\\n",
    "            'To sharing word embedding the src/trg word2idx table shall be the same.'\n",
    "\n",
    "    fields = {'src': data['vocab']['src'], 'trg':data['vocab']['trg']}\n",
    "\n",
    "    train = Dataset(examples=data['train'], fields=fields)\n",
    "    val = Dataset(examples=data['valid'], fields=fields)\n",
    "\n",
    "    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n",
    "    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n",
    "\n",
    "    return train_iterator, val_iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, cuda=True, d_inner_hid=2048, d_k=64, d_model=512, d_v=64, d_word_vec=512, data_pkl='m30k_deen_shr.pkl', dropout=0.1, embs_share_weight=True, epoch=2, label_smoothing=True, log=None, max_token_seq_len=100, n_head=8, n_layers=6, n_warmup_steps=128000, no_cuda=False, proj_share_weight=True, save_mode='best', save_model='trained', src_pad_idx=1, src_vocab_size=9521, train_path=None, trg_pad_idx=1, trg_vocab_size=9521, val_path=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :   0%|                                                                      | 0/1813 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 0 ]\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 21])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 21])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 21])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 21])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 21])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 21])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 27])\n",
      "scaleddpa torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 20])\n",
      "scaleddpa torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 20])\n",
      "scaleddpa torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 20])\n",
      "scaleddpa torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 20])\n",
      "scaleddpa torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 20])\n",
      "scaleddpa torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 20])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 20])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 20])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 20])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 20])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 20])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 21])\n",
      "scaleddpa torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 20, 64]) torch.Size([16, 8, 20, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 21, 64]) torch.Size([16, 8, 21, 20])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 19, 64]) torch.Size([16, 8, 19, 19])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 18, 64]) torch.Size([16, 8, 18, 18])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 25])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 23])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 25])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 23])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 25])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 23])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 25])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 23])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 25])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 23])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 25])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 23])\n",
      "scaleddpa torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 25, 64]) torch.Size([16, 8, 25, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 23, 64]) torch.Size([16, 8, 23, 25])\n",
      "scaleddpa torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 32])\n",
      "scaleddpa torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 32])\n",
      "scaleddpa torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 32])\n",
      "scaleddpa torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 32])\n",
      "scaleddpa torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 32])\n",
      "scaleddpa torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 32])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 28])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 32])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 28])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 32])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 28])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 32])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 28])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 32])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 28])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 32])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 28])\n",
      "scaleddpa torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 32, 64]) torch.Size([16, 8, 32, 64])\n",
      "scaleddpa out: torch.Size([16, 8, 28, 64]) torch.Size([16, 8, 28, 32])\n",
      "scaleddpa torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64]) torch.Size([16, 8, 27, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d0ff7bc42777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;34m'-proj_share_weight'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-label_smoothing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-save_model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trained'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             '-b', '16', '-warmup', '128000', '-epoch', '2']\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-1bab81b15db1>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    256\u001b[0m         2.0, opt.d_model, opt.n_warmup_steps)\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1bab81b15db1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, training_data, validation_data, optimizer, device, opt)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         train_loss, train_accu = train_epoch(\n\u001b[1;32m--> 145\u001b[1;33m             model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mprint_performances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1bab81b15db1>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, training_data, optimizer, opt, device, smoothing)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# backward and update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Users\\Master\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\MyFiles\\Classes\\2020 Spring\\cs690\\project\\janossy_lstm_attention\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src_seq, trg_seq)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mtrg_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pad_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg_pad_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mget_subsequent_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[0mdec_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mseq_logit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg_word_prj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_logit_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Users\\Master\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\MyFiles\\Classes\\2020 Spring\\cs690\\project\\janossy_lstm_attention\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src_seq, src_mask, return_attns)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menc_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_stack\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_slf_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslf_attn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m             \u001b[0menc_slf_attn_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0menc_slf_attn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_attns\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Users\\Master\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\MyFiles\\Classes\\2020 Spring\\cs690\\project\\janossy_lstm_attention\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, enc_input, slf_attn_mask)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslf_attn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         enc_output, enc_slf_attn = self.slf_attn(\n\u001b[1;32m--> 148\u001b[1;33m             enc_input, enc_input, enc_input, mask=slf_attn_mask)\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[0menc_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_ffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_slf_attn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Users\\Master\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\MyFiles\\Classes\\2020 Spring\\cs690\\project\\janossy_lstm_attention\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# For head axis broadcasting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m# Transpose to move the head dimension back: b x lq x n x dv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Users\\Master\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\MyFiles\\Classes\\2020 Spring\\cs690\\project\\janossy_lstm_attention\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scaleddpa'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['train.py', '-data_pkl', 'm30k_deen_shr.pkl', '-embs_share_weight', \n",
    "            '-proj_share_weight', '-label_smoothing', '-save_model', 'trained', \n",
    "            '-b', '16', '-warmup', '128000', '-epoch', '2']\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
